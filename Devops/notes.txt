on AWS server it prevents to start the process on ports like 80,443,13,etc. OS prevents.
Can use sudo to run the process as root.


SO we will use nginx to run the process on port 80 which will redirect to the process running on port 8080 or other ports depending on the request.

sudo vi /etc/nginx/nginx.conf

sudo nginx -s reload


Docker and Containers
Containers are a way to package and distribute software applications in a way that makes them easy to deploy and run consistently across different environments. They allow you to package an application, along with all its dependencies and libraries, into a single unit that can be run on any machine with a container runtime, such as Docker.

Docker Images vs Containers
A Docker image is a lightweight, standalone executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. It is designed to be used as a base for building other Docker images.
A container is a running instance of an image that runs in a Docker container runtime.

image- everything needed to run a piece of software
container- running instance of an image

docker run -p 27017:27017 mongo
we run the container here.
it first finds whether the image is already present on the system or not.
if not it will download the image from the registry.
after that it will run the container.

docker images list all the images present on the system.
docker ps list all the containers present on the system.

docker run -p 27017:27017 mongo
machine's port 27017 is mapped to the container's port 27017.
ports:
0.0.0.0:27017->27017/tcp
docker run -p 27018:27017 mongo
machine's port 27018 is mapped to the container's port 27017.
ports:
0.0.0.0:27018->27017/tcp


docker exec -it mongodb sh
this will run the command in the container. -it means interactive and tty.

docker build -t mongo .
this will build the image from the Dockerfile.


In the dockerfile
COPY . .

RUN npm install
# RUN npm run build

EXPOSE 3000
# till here the image is created
CMD ["node", "index.js"]

# only one command can be run in the dockerfile and it will be executed when the container is created


even if we change the code slightly npm install runs again even though the package.json is not changed.
only the steps above these are cached.

now to avoid this and optimise.
 So we will use something like this:


 COPY ./package.json ./package.json
 COPY ./package-lock.json ./package-lock.json
 RUN npm install
 COPY . .
 RUN npm run build
 EXPOSE 3000
    
this will copy the package.json and package-lock.json to the container and install the dependencies.
then it will copy the rest of the files to the container and run the build command.

now if we change the code slightly npm install will not run again.
only the steps above these are cached. 



Docker Containers are stateless and doesn't retain any data.
They are created from an image and then they are destroyed when the container is destroyed.

To retain data we need to use volumes.
Volumes are directories that are mounted from the host machine to the container.
The data in the volume is not deleted when the container is destroyed.

 docker run -p 27017:27017 -v mongo_db_data:/data/db mongo

 docker network create my-network
 docker network for letting 2 containers to communicate with each other

  docker run -p 27017:27017 --network my-network -v mongo_db_data:/data/db mongo 

  now in the other container we can connect to the mongodb using the hostname
  mongodb://mongo:27017 



docker run -e POSTGRES_PASSWORD=mysecretpwd -d -p 5432:5432 postgres